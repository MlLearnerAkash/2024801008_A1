# Assignment-1: Introduction to NLP, IIIT, Hyderabad
This project implements the three tokenizers(viz. WhiteSpace, Regex and BPE) and langauge models(viz. Witten-Bell and Kneser-Ney).


## ðŸ“‚ File Structure

The project is organized as follows:

```text
Assignment-1/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ corpora/                # Raw corpus files
â”‚   â”‚   â”œâ”€â”€ cc100_en.jsonl
â”‚   â”‚   â””â”€â”€ cc100_mn.jsonl
â”‚   â””â”€â”€ partitions/             # Data splits
â”‚       â”œâ”€â”€ train.txt
â”‚       â”œâ”€â”€ val.txt
â”‚       â””â”€â”€ test.txt
â”œâ”€â”€ environment.yaml            # Conda environment configuration
â”œâ”€â”€ language_models.py          # Language model architecture classes
â”œâ”€â”€ tokenizers.py               # Tokenizer logic (BPE) and training script
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ .gitignore                  # Files to ignore (e.g., __pycache__, large data)
```



## To clean and tokenize
```bash
python tokenizers.py --mode {corpus_clean,tokenization} --tokenizer {WhitespaceTokenizer, RegexTokenizer, BPETokenizer} --input dataset/corpora/cc100_{en, mn}.jsonl --output dataset/corpora/partitions/mongolean --train_ratio 0.8 --val_ratio 0.1 --test_ratio 0.1
```

## To run langauge models
```bash
python language_models.py --train dataset/corpora/partitions/train.txt --test dataset/corpora/partitions/test.txt --tokenizer {whitespace, regex, bpe} --smoothing {none, witten-bell, kneser-ney} 
```